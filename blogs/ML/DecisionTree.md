---
layout: page
permalink: /blogs/ML/DecisionTree/index.html
title: 决策树
---

## 决策树

### 1.概述

<br>

决策树算法是一种有效的分类方法，通过树形结构和层层推理实现最终分类。与基于概率推断的朴素贝叶斯分类器和逻辑回归模型不同，决策树不依赖于先验条件，因此更适合用于探索式知识发现。<br>

决策树的分类过程更接近人类的判断机制。例如，在购房时，购房者通常首先考虑每平方米的价格，价格合适后再考虑面积、位置等因素。在这一过程中，购房者会根据不同的特征逐步做出“买”或“不买”的决策。决策树通过这种方式形式化了决策过程，并引入量化指标，形成了一种分类算法。<br>

决策树由**根节点、内部节点和叶节点**组成。根节点代表样本全集，内部节点对应特征属性的测试，叶节点则表示最终的决策结果。决策过程从根节点开始，测试待分类项的特征属性，并根据其值选择相应的内部节点，直到到达某个叶节点，最终得出分类结果。<br>

决策树的学习过程包括**特征选择、树生成和剪枝**。**特征选择的目的是确定哪些特征对分类结果最为重要**，从而有效划分特征空间。理想的特征选择应确保每次划分后，分支节点的样本尽可能属于同一类别。<br>

在特征选择中，通常使用信息增益作为准则。信息增益是信息论的核心概念，描述了**在已知特征后对数据分类不确定性的减少程度**。信息增益越大，特征的分类能力越强。特征选择的过程是自顶向下的，每次划分时计算各特征的信息增益，选择最大值。<br>

例如，在银行贷款审批中，申请人的特征如年龄、工作状态、房产情况等会影响是否批准贷款。如果某个特征（如是否有房产）完全决定了分类结果，那么该特征的信息增益最大。在这种情况下，仅需依据该特征即可做出决策。而如果特征（如年龄）与贷款结果无关，则其信息增益小，分类能力弱。<br>

下图展示了一个简单的决策树示例，用于判断是否接受某一工作，根节点为月薪，叶子节点为是否接受该工作<br>

<center>
<img src="https://qilu-yuan.github.io/Figure/DecisionTree/1.jpg" width="500" height="500" alt="AltText" />
</center>

---

### 2.ID3算法

<br>

最早的决策树算法——**ID3算法**，利用信息增益选择特征构建决策树。该算法从根节点出发，计算所有特征的信息增益，选择增益最大的特征作为节点特征，递归构建子节点，直到信息增益很小或没有特征可供选择。<br>

然而，**ID3算法存在对多值属性偏好的问题**，可能影响泛化能力。这促使研究者提出了C4.5算法，改进了特征选择方法。**C4.5算法引入信息增益比**，作为选择最优划分属性的依据，确保对多值和少值属性一视同仁。<br>

此外，CART算法则使用基尼系数替代熵模型进行特征选择，提供了另一种有效的决策树构建方法。<br>

---

### 3.CART算法

<br>

CART算法，即分类与回归树（Classification and Regression Tree），既可用于分类也可用于回归。假设数据中有 *K* 个类别，第 *k* 个类别的概率为 $p_k$，则基尼系数为 
$$
Gini=1-\sum^K p_k^2
$$
简单来说，基尼系数衡量了**随机选择两个样本属于不同类别的概率**。基尼系数越小，表示数据集的纯度越高。<br>相较于熵模型，基尼系数避免了对数运算，提高了执行效率。<br>

对于回归问题，CART使用平方误差作为划分标准。假设数据集 *D* 被划分为两个子集 *D1* 和 *D2*，则平方误差定义为
$$
SE(D)=\sum_{i \in D}(y_i-\bar{y})^2
$$
$$
SE(D1,D2)=SE(D1)+SE(D2)
$$
其中，$y_i$ 为样本的实际值，$\bar{y}$ 为样本的平均值。<br>


CART算法每次对特征值进行二分，而非多分，最终生成**二叉树模型**。在计算基尼系数时，需找到使其最小的最优切分点。树生成时，根据最优特征和切分点创建两个子节点，并将训练数据分配到子节点中。例如，对于特征A的三种类别 A1、A2 和 A3，CART会考虑 {A1}/{A2, A3}、{A2}/{A1, A3} 和 {A3}/{A1, A2} 三种分法，以找到基尼系数会误差最小的组合。<br>

---

### 4.CART分类树的构建过程
<br>
CART分类树的构建过程包括以下步骤：
1. **初始化根节点**：将所有训练样本放入根节点。<br>
2. **计算基尼系数**：对于每个特征，计算其所有可能切分点的基尼系数(平方误差)，选择使基尼系数(平方误差)最小的切分点作为该特征的最优切分点。<br>
3. **选择最优特征**：在所有特征中，选择基尼系数或平方误差最小的特征作为当前节点的划分特征。<br>
4. **划分数据集**：根据最优特征的切分点，将数据集划分为两个子集，分别放入左子节点和右子节点。<br>
5. **递归构建子节点**：对每个子节点重复步骤2至步骤4，直到满足停止条件（如节点纯度达到一定阈值或样本数小于预设值）。<br>
6. **生成叶节点**：当达到停止条件时，将节点标记为叶节点，并根据多数类原则确定其类别标签。<br>

<center>
<img src="https://qilu-yuan.github.io/Figure/DecisionTree/2.jpg" width="500" height="500" alt="AltText" />
</center>

上图是一个例子，展示了CART分类树的构建过程。该例子为100个样本的二分类过程，通过若干次特征划分，成功分离出了两种类别<br>

另外，CART回归树的构建过程与分类树类似，只是使用平方误差作为划分标准。<br>



### 4.过拟合与剪枝

<br>

决策树同其他机器学习算法一样，面临过拟合的问题，**“剪枝”是其主要对抗手**段。通过主动去掉分支，剪枝可以降低过拟合风险，提升模型的泛化性能。<br>

判定泛化性能提升的方法是**定义损失函数并极小化，类似于使用正则化的最大似然估计进行模型选择**。另一种简单的方法是从训练数据集中抽取一部分用于验证，根据验证集的分类精度变化决定是否剪枝。<br>

**决策树的剪枝策略分为预剪枝和后剪枝。**预剪枝在生成过程中对每个节点进行估计，若当前划分无法提升泛化性能，则直接将其标记为叶节点。这种方法可以减少决策树的时间开销，但可能导致“误伤”，移除某些潜在有用的分支，增加欠拟合风险。<br>

后剪枝则是在生成完整决策树后，计算其在验证集上的精度，再进行剪枝。通过比较剪枝前后的精度决定是否保留分支。后剪枝通常能保留更多分支，欠拟合风险较小，但训练开销较大。<br>

决策树后剪枝一般采取CPP（Cost-Complexity Pruning）方法。该方法通过引入复杂度参数 $\alpha$，在损失函数中平衡训练误差和树的复杂度。
损失函数定义为
$$
R_\alpha(T)=R(T)+\alpha|T|
$$
其中，$R(T)$ 为决策树 *T* 在验证集上的误差率，$|T|$ 为树的叶节点数，$\alpha$ 为复杂度参数。通过调整 $\alpha$，可以控制剪枝的程度。<br>
具体步骤如下：<br>
1. **生成完整决策树**：首先构建一棵完整的决策树。<br>
2. **计算子树序列**：通过逐步剪枝，生成一系列子树，每次剪掉一个分支节点。<br>
3. **计算损失函数**：对于每棵子树，计算其在验证集上的损失函数值 $R_\alpha(T)$。<br>
4. **选择最优子树**：根据损失函数值，选择使 $R_\alpha(T)$ 最小的子树作为最终模型。<br>


---