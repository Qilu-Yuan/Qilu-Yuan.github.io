---
layout: page
permalink: /blogs/ML/DecisionTree/index.html
title: 决策树
---

## 决策树

### 1.概述

<br>

决策树算法是一种有效的分类方法，通过树形结构和层层推理实现最终分类。与基于概率推断的朴素贝叶斯分类器和逻辑回归模型不同，决策树不依赖于先验条件，因此更适合用于探索式知识发现。<br>

决策树的分类过程更接近人类的判断机制。例如，在购房时，购房者通常首先考虑每平方米的价格，价格合适后再考虑面积、位置等因素。在这一过程中，购房者会根据不同的特征逐步做出“买”或“不买”的决策。决策树通过这种方式形式化了决策过程，并引入量化指标，形成了一种分类算法。<br>

决策树由**根节点、内部节点和叶节点**组成。根节点代表样本全集，内部节点对应特征属性的测试，叶节点则表示最终的决策结果。决策过程从根节点开始，测试待分类项的特征属性，并根据其值选择相应的内部节点，直到到达某个叶节点，最终得出分类结果。<br>

决策树的学习过程包括**特征选择、树生成和剪枝**。**特征选择的目的是确定哪些特征对分类结果最为重要**，从而有效划分特征空间。理想的特征选择应确保每次划分后，分支节点的样本尽可能属于同一类别。<br>

在特征选择中，通常使用信息增益作为准则。信息增益是信息论的核心概念，描述了**在已知特征后对数据分类不确定性的减少程度**。信息增益越大，特征的分类能力越强。特征选择的过程是自顶向下的，每次划分时计算各特征的信息增益，选择最大值。<br>

例如，在银行贷款审批中，申请人的特征如年龄、工作状态、房产情况等会影响是否批准贷款。如果某个特征（如是否有房产）完全决定了分类结果，那么该特征的信息增益最大。在这种情况下，仅需依据该特征即可做出决策。而如果特征（如年龄）与贷款结果无关，则其信息增益小，分类能力弱。<br>

---

### 2.ID3算法

<br>

最早的决策树算法——**ID3算法**，利用信息增益选择特征构建决策树。该算法从根节点出发，计算所有特征的信息增益，选择增益最大的特征作为节点特征，递归构建子节点，直到信息增益很小或没有特征可供选择。<br>

然而，**ID3算法存在对多值属性偏好的问题**，可能影响泛化能力。这促使研究者提出了C4.5算法，改进了特征选择方法。**C4.5算法引入信息增益比**，作为选择最优划分属性的依据，确保对多值和少值属性一视同仁。<br>

此外，CART算法则使用基尼系数替代熵模型进行特征选择，提供了另一种有效的决策树构建方法。<br>

---

### 3.CART算法

<br>

CART算法，即分类与回归树（Classification and Regression Tree），既可用于分类也可用于回归。假设数据中有 *K* 个类别，第 *k* 个类别的概率为 $p_k$，则基尼系数为 $1-\sum^K p_k^2$。相较于熵模型，基尼系数避免了对数运算，提高了执行效率。<br>

CART算法每次对特征值进行二分，而非多分，最终生成**二叉树模型**。在计算基尼系数时，需找到使其最小的最优切分点。树生成时，根据最优特征和切分点创建两个子节点，并将训练数据分配到子节点中。例如，对于特征A的三种类别 A1、A2 和 A3，CART会考虑 {A1}/{A2, A3}、{A2}/{A1, A3} 和 {A3}/{A1, A2} 三种分法，以找到基尼系数最小的组合。<br>

---

### 4.过拟合

<br>

决策树同其他机器学习算法一样，面临过拟合的问题，**“剪枝”是其主要对抗手**段。通过主动去掉分支，剪枝可以降低过拟合风险，提升模型的泛化性能。<br>

判定泛化性能提升的方法是**定义损失函数并极小化，类似于使用正则化的最大似然估计进行模型选择**。另一种简单的方法是从训练数据集中抽取一部分用于验证，根据验证集的分类精度变化决定是否剪枝。<br>

**决策树的剪枝策略分为预剪枝和后剪枝。**预剪枝在生成过程中对每个节点进行估计，若当前划分无法提升泛化性能，则直接将其标记为叶节点。这种方法可以减少决策树的时间开销，但可能导致“误伤”，移除某些潜在有用的分支，增加欠拟合风险。<br>

后剪枝则是在生成完整决策树后，计算其在验证集上的精度，再进行剪枝。通过比较剪枝前后的精度决定是否保留分支。后剪枝通常能保留更多分支，欠拟合风险较小，但训练开销较大。<br>

---