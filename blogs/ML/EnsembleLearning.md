---
layout: page
permalink: /blogs/ML/EnsembleLearning/index.html
title: 集成学习
---

## 集成学习

### **1. 集成学习基础**

<br>

(1)核心思想：结合多个学习器（同质或异质）以获得优于单一模型的性能。<br>

**同质集成**：相同类型学习器（如随机森林中的多棵决策树）。<br>

**异质集成**：不同类型学习器组合（如SVM、决策树、神经网络混合）。<br>

(2)关键要求：<br>

**准确性**：个体学习器需具备一定性能，避免“垃圾进，垃圾出”。<br>

**多样性**：学习器间误差需互补，类似MIMO分集，需独立或低相关性。<br>

---

### **2. 集成方法分类**

#### **(1) 序列化方法：Boosting**

<br>

- **代表算法**：AdaBoost（自适应提升）。<br>

- 核心机制：<br>

**样本权重调整**：每轮训练后增加错误样本权重，迫使后续学习器专注难例。<br>

**弱分类器组合**：按分类错误率加权投票，错误率低的学习器权重更大。<br>

**损失函数**：指数损失函数，通过前向分步算法逐步优化。<br>

- 特点：<br>

降低模型偏差，适合弱学习器（如决策树桩）。<br>

模型逐步调整，自适应优化困难样本。<br>

#### **(2) 并行化方法：Bagging**

<br>

- **代表算法**：随机森林（Random Forest）。<br>

- 核心机制：<br>
**自助采样（Bootstrap）**：有放回抽样生成训练子集，36.8%样本作为包外估计（OOB）。<br>

**随机特征选择**：每个节点随机选取*k*个属性(如k=d)，确保决策树多样性。<br>

**多数投票**：最终结果取个体学习器预测的众数或均值。<br>

- 特点：<br>

降低模型方差，适合高方差基学习器（如深决策树）。<br>

包外误差可无偏估计泛化性能，无需额外验证集。<br>

---

### **3. 泛化误差分析**

<br>

- **Boosting**：通过全量数据迭代优化，降低平均偏差，提升弱学习器性能。<br>

- **Bagging**：子集采样与模型平均降低方差，对异常值鲁棒。<br>

---

### **4. 关键细节与对比**

| **方面**       | **Boosting（AdaBoost）**     | **Bagging（随机森林）**    |
| -------------- | ---------------------------- | -------------------------- |
| **数据使用**   | 全量数据，样本权重动态调整   | 自助采样，各子集独立       |
| **学习顺序**   | 串行生成，依赖前序结果       | 并行生成，学习器独立       |
| **多样性来源** | 样本权重变化导致关注不同样本 | 数据子集和特征随机选择     |
| **误差优化**   | 降低偏差（弱→强学习器）      | 降低方差（平滑模型波动）   |
| **典型应用**   | 二分类、多分类任务           | 分类、回归、特征重要性评估 |

---

### **5. 实践建议**

<br>

- 选择策略：<br>

高偏差问题（欠拟合）→ Boosting（如AdaBoost）。<br>

高方差问题（过拟合）→ Bagging（如随机森林）。<br>

- 参数调优：<br>
**AdaBoost**：基学习器数量、学习率（缩减权重更新步长）。<br>

**随机森林**：树的数量、最大深度、特征子集大小k*k*。<br>

- **异质集成**：考虑Stacking等复杂方法，但需权衡计算成本与性能增益。<br>

---

### **6. 总结**

<br>

集成学习使用多个个体学习器来获得比每个单独学习器更好的预测性能，包括序列化方法和并行化方法两类；<br>

多样性要求集成学习中的不同个体学习器之间具有足够的差异性；<br>

序列化方法采用 Boosting 机制，通过重复使用概率分布不同的训练数据实现集成，可以降低泛化误差中的偏差；<br>

并行化方法采用 Bagging 机制，通过在训练数据中多次自助抽取不同的采样子集实现集成，可以降低泛化误差中的方差。<br>

---