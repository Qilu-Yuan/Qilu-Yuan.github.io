---
layout: page
permalink: /blogs/ML/SupportVectorMachine/index.html
title: 支持向量机
---

## 支持向量机

### 线性可分支持向量机

<br>

支持向量机是一种二分类算法，通过在高维空间中构造超平面来实现样本的分类。最简单的情况是线性可分的训练数据，此时称为线性可分支持向量机。<br>

在二维平面中，若某些点位于 $x$ 轴上方，另一些点位于 $x$ 轴下方，这样的数据集就是线性可分的，$x$ 轴为分界线。若假设上方点在 $y=1$ 及其上方，下方点在 $y=-2$ 及其下方，则任何平行于 $x$ 轴且在 $(-2, 1)$ 之间的直线都可以分开这些点。最优的分界线是 $y=-0.5$，它能够最大化两个类别的间隔，具有更强的泛化能力。<br>

在高维的特征空间上，划分超平面可以用简单的线性方程描述<br>

$$
\mathtt{w}^T \cdot \mathtt{x} +b = 0
$$

<br>

式子中n维向量**w**为法向量，$b$ 为截距。样本点到超平面的距离称为几何间隔，表示为：<br>

$$
r=\frac{\mathtt{w}^T \cdot \mathtt{x} +b}{\vert \vert\mathtt{w}\vert \vert}
$$

<br>

通过合理设置参数，可以使每个样本点到超平面的距离不小于 -1，形成以下约束：<br>

$$
\mathtt{w}^T \cdot \mathtt{x_i} +b \geq 1,y_i = +1 \\
\mathtt{w}^T \cdot \mathtt{x_i} +b \leq -1,y_i = -1 \\
$$

<br>

距离超平面最近的样本点称为“支持向量”，它们的任务是最大化 $2/\vert \vert\mathtt{w}\vert \vert$，等价于最小化 $1/2\vert \vert\mathtt{w}\vert \vert^2$。<br>

---

### 线性支持向量机

<br>

线性可分支持向量机旨在最大化硬间隔，但实际数据中常存在噪声，导致线性不可分。为解决此问题，线性支持向量机引入了软间隔的概念。<br>

在软间隔最大化中，需对每个样本引入松弛变量 $\xi_i \geq 0$，使约束条件变为：<br>

$$
\mathtt{w}^T \cdot \mathtt{x_i} + b \geq 1 - \xi_i,\ y_i = +1 \\
\mathtt{w}^T \cdot \mathtt{x_i} + b \leq -1 + \xi_i,\ y_i = -1
$$

<br>

目标函数变为：<br>

$$
\frac{1}{2}||\mathtt{w}||^2 + C\sum_{i=1}^N \xi_i
$$

<br>

其中 $C > 0$ 为惩罚参数，控制误分类的惩罚力度。

---

### 核技巧

<br>

线性可分支持向量机和线性支持向量机无法处理非线性问题。通过将样本映射到高维特征空间，可以将非线性问题转化为线性问题。这一过程称为核技巧。<br>

核函数的形式为：<br>

$$
K(x,z) = \phi(x) \cdot \phi(z)
$$

<br>

核函数的特点是计算在低维空间中完成，避免高维空间的复杂计算。常用的核函数包括：<br>

$$
线性核:K(\mathtt{X,Y})=\mathtt{X^T Y}\\
多项式核:K(\mathtt{X,Y})=(\mathtt{X^T Y}+c)^d\ ,c为常数，d≥1 为多项式次数\\
高斯核:K(\mathtt{X,Y})=\exp\left( -\frac{\mathtt{||X-Y||^2}}{2\sigma^2} \right) ,\sigma>0为高斯核的带宽\\
拉普拉斯核:K(\mathtt{X,Y})=\exp\left( -\frac{\mathtt{||X-Y||}}{\sigma} \right) ,\sigma>0\\
Sigmoid核:K(\mathtt{X,Y})=\tanh(\mathtt{\beta X^T Y}+\theta), \beta>0, \theta<0
$$

<br>

核函数的选择对支持向量机的性能至关重要。<br>

---

### SMO 算法

<br>

在实现支持向量机时，常面临全局最优解难以求得的挑战。SMO（序列最小优化）算法通过将原始的二次规划问题分解为仅有两个变量的子问题并求解，来高效实现支持向量机。<br>

支持向量机的一个重要特性是，最终模型只与支持向量相关，这也是其名称的由来。**支持向量机这个名字强调了这类算法的关键是如何根据支持向量构建出解，算法的复杂度也主要取决于支持向量的数目**。<br>

---

### 总结

<br>

- 线性可分支持向量机通过硬间隔最大化求出划分超平面，解决线性分类问题；<br>
- 线性支持向量机通过软间隔最大化求出划分超平面，解决线性分类问题；<br>
- 非线性支持向量机利用核函数实现从低维原始空间到高维特征空间的转换，在高维空间上解决非线性分类问题；<br>
- 支持向量机的学习是个凸二次规划问题，可以用 SMO 算法快速求解。<br>

---